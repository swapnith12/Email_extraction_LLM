###
Testing llm integration into code first 
I have used gorq client and init openai model 
tested with simple prompt 
user:"say hello"
llm:"Hello! I hope you're having a wonderful day."

Scraping only output i want from llm 
badoutput:"here is your answer {intercom :"",}"
good output:{intercom:""}
prepared util.py file applied my logic there 